{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b38df7d",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e266eca-b017-461f-9be4-bec02cae9b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:23:16.915050Z",
     "iopub.status.busy": "2024-03-02T14:23:16.914878Z",
     "iopub.status.idle": "2024-03-02T14:23:16.929039Z",
     "shell.execute_reply": "2024-03-02T14:23:16.928501Z"
    },
    "papermill": {
     "duration": 0.020755,
     "end_time": "2024-03-02T14:23:16.930477",
     "exception": false,
     "start_time": "2024-03-02T14:23:16.909722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae8a7a9-6902-424e-8e66-b107fccb361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:23:16.937812Z",
     "iopub.status.busy": "2024-03-02T14:23:16.937471Z",
     "iopub.status.idle": "2024-03-02T14:23:19.956272Z",
     "shell.execute_reply": "2024-03-02T14:23:19.955061Z"
    },
    "papermill": {
     "duration": 3.023656,
     "end_time": "2024-03-02T14:23:19.957791",
     "exception": false,
     "start_time": "2024-03-02T14:23:16.934135",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from llamawrapper import load_unemb_only, LlamaHelper\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from utils import plot_ci, plot_ci_plus_heatmap\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "sys.path.append('../tuned-lens')\n",
    "from tuned_lens.nn.lenses import TunedLens,LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb93987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:23:19.966679Z",
     "iopub.status.busy": "2024-03-02T14:23:19.966097Z",
     "iopub.status.idle": "2024-03-02T14:23:20.012872Z",
     "shell.execute_reply": "2024-03-02T14:23:20.011565Z"
    },
    "papermill": {
     "duration": 0.052965,
     "end_time": "2024-03-02T14:23:20.014839",
     "exception": false,
     "start_time": "2024-03-02T14:23:19.961874",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "input_lang = 'zh'\n",
    "target_lang = 'fr'\n",
    "model_size = '7b'\n",
    "prefix = \"./data/langs/\"\n",
    "type = 'tuned-chinese-en'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757184b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:23:20.026466Z",
     "iopub.status.busy": "2024-03-02T14:23:20.026129Z",
     "iopub.status.idle": "2024-03-02T14:23:20.067004Z",
     "shell.execute_reply": "2024-03-02T14:23:20.066166Z"
    },
    "papermill": {
     "duration": 0.050106,
     "end_time": "2024-03-02T14:23:20.068621",
     "exception": false,
     "start_time": "2024-03-02T14:23:20.018515",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_size = \"6.9b\"\n",
    "target_lang = \"ru\"\n",
    "input_lang = \"fr\"\n",
    "type = \"tuned\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb0899b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:23:20.082139Z",
     "iopub.status.busy": "2024-03-02T14:23:20.081869Z",
     "iopub.status.idle": "2024-03-02T14:23:20.133544Z",
     "shell.execute_reply": "2024-03-02T14:23:20.132788Z"
    },
    "papermill": {
     "duration": 0.062815,
     "end_time": "2024-03-02T14:23:20.135019",
     "exception": false,
     "start_time": "2024-03-02T14:23:20.072204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_en_input = pd.read_csv(f'{prefix}{input_lang}/clean.csv').reindex()\n",
    "df_en_target = pd.read_csv(f'{prefix}{target_lang}/clean.csv').reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af501385",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33747213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:23:20.145566Z",
     "iopub.status.busy": "2024-03-02T14:23:20.144960Z",
     "iopub.status.idle": "2024-03-02T14:23:23.843573Z",
     "shell.execute_reply": "2024-03-02T14:23:23.842358Z"
    },
    "papermill": {
     "duration": 3.703731,
     "end_time": "2024-03-02T14:23:23.844547",
     "exception": true,
     "start_time": "2024-03-02T14:23:20.140816",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:01<00:01,  1.75s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.29s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params.pt AlignmentResearch/tuned-lens space main lens/EleutherAI/6.9b None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not find lens at the specified resource id. Available lensresources are: lmsys/vicuna-13b-v1.1, EleutherAI/pythia-12b-deduped-v0, gpt2-xl, EleutherAI/pythia-160m-deduped-v0, gpt2, EleutherAI/pythia-12b-deduped, gpt2-large, EleutherAI/pythia-6.9b-deduped, EleutherAI/pythia-1.4b-deduped-v0, EleutherAI/pythia-410m-deduped, EleutherAI/gpt-neox-20b, EleutherAI/pythia-1.4b-deduped, facebook/llama-7b, facebook/llama-65b, meta-llama/Llama-2-7b-chat-hf/hh-rlhf, meta-llama/Llama-2-7b-chat-hf, facebook/opt-1.3b, meta-llama/Llama-2-13b-chat-hf, EleutherAI/pythia-70m-deduped, EleutherAI/pythia-1b-deduped-v0, facebook/llama-30b, meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-chat-hf/hh-rlhf, facebook/opt-6.7b, facebook/opt-125m, EleutherAI/pythia-160m-deduped, EleutherAI/pythia-2.8b-deduped, meta-llama/Llama-2-13b-hf, EleutherAI/pythia-70m-deduped-v0, CarperAI/stable-vicuna-13b, EleutherAI/pythia-2.8b-deduped-v0, EleutherAI/pythia-410m-deduped-v0, EleutherAI/pythia-6.9b-deduped-v0, facebook/llama-13b",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/spaces/AlignmentResearch/tuned-lens/resolve/main/lens/EleutherAI/6.9b/params.pt",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/llm-latent-language/../tuned-lens/tuned_lens/load_artifacts.py:94\u001b[0m, in \u001b[0;36mload_lens_artifacts\u001b[0;34m(resource_id, repo_id, repo_type, revision, config_file, ckpt_file, subfolder, cache_dir)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(ckpt_file, repo_id, repo_type,revision,resource_folder, cache_dir)\n\u001b[0;32m---> 94\u001b[0m params_path \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresource_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    102\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m config_path \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    105\u001b[0m     filename\u001b[38;5;241m=\u001b[39mconfig_file,\n\u001b[1;32m    106\u001b[0m     repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m     force_download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    112\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/file_download.py:1667\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1676\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:315\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEntry Not Found for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m EntryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGatedRepo\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mEntryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-65e3365b-4bc2f36261c869226d5bfa77;a23580bf-bcc2-4b1f-ae99-e5f989a8d1eb)\n\nEntry Not Found for url: https://huggingface.co/spaces/AlignmentResearch/tuned-lens/resolve/main/lens/EleutherAI/6.9b/params.pt.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m     out_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./visuals/logit\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtuned\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m     llama_lens \u001b[38;5;241m=\u001b[39m \u001b[43mTunedLens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_model_and_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllama\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEleutherAI/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m     out_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./visuals/tuned\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     17\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEleutherAI/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,local_files_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m,cache_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/ssd-1/hf_cache/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m             )\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/llm-latent-language/../tuned-lens/tuned_lens/nn/lenses.py:231\u001b[0m, in \u001b[0;36mTunedLens.from_model_and_pretrained\u001b[0;34m(cls, model, lens_resource_id, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lens_resource_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     lens_resource_id \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname_or_path\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_unembed_and_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mUnembed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlens_resource_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/llm-latent-language/../tuned-lens/tuned_lens/nn/lenses.py:257\u001b[0m, in \u001b[0;36mTunedLens.from_unembed_and_pretrained\u001b[0;34m(cls, unembed, lens_resource_id, **kwargs)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Validate kwargs\u001b[39;00m\n\u001b[1;32m    255\u001b[0m load_artifact_varnames \u001b[38;5;241m=\u001b[39m load_artifacts\u001b[38;5;241m.\u001b[39mload_lens_artifacts\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames\n\u001b[0;32m--> 257\u001b[0m config_path, ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[43mload_artifacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_lens_artifacts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresource_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlens_resource_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mload_artifact_varnames\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     config \u001b[38;5;241m=\u001b[39m TunedLensConfig\u001b[38;5;241m.\u001b[39mfrom_dict(json\u001b[38;5;241m.\u001b[39mload(f))\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/llm-latent-language/../tuned-lens/tuned_lens/load_artifacts.py:126\u001b[0m, in \u001b[0;36mload_lens_artifacts\u001b[0;34m(resource_id, repo_id, repo_type, revision, config_file, ckpt_file, subfolder, cache_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m     available_lenses \u001b[38;5;241m=\u001b[39m available_lens_artifacts(\n\u001b[1;32m    115\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m    116\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    120\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find lens at the specified resource id. Available lens\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresources are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(available_lenses)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m     )\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m params_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Path(config_path), Path(params_path)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not find lens at the specified resource id. Available lensresources are: lmsys/vicuna-13b-v1.1, EleutherAI/pythia-12b-deduped-v0, gpt2-xl, EleutherAI/pythia-160m-deduped-v0, gpt2, EleutherAI/pythia-12b-deduped, gpt2-large, EleutherAI/pythia-6.9b-deduped, EleutherAI/pythia-1.4b-deduped-v0, EleutherAI/pythia-410m-deduped, EleutherAI/gpt-neox-20b, EleutherAI/pythia-1.4b-deduped, facebook/llama-7b, facebook/llama-65b, meta-llama/Llama-2-7b-chat-hf/hh-rlhf, meta-llama/Llama-2-7b-chat-hf, facebook/opt-1.3b, meta-llama/Llama-2-13b-chat-hf, EleutherAI/pythia-70m-deduped, EleutherAI/pythia-1b-deduped-v0, facebook/llama-30b, meta-llama/Llama-2-7b-hf, meta-llama/Llama-2-13b-chat-hf/hh-rlhf, facebook/opt-6.7b, facebook/opt-125m, EleutherAI/pythia-160m-deduped, EleutherAI/pythia-2.8b-deduped, meta-llama/Llama-2-13b-hf, EleutherAI/pythia-70m-deduped-v0, CarperAI/stable-vicuna-13b, EleutherAI/pythia-2.8b-deduped-v0, EleutherAI/pythia-410m-deduped-v0, EleutherAI/pythia-6.9b-deduped-v0, facebook/llama-13b"
     ]
    }
   ],
   "source": [
    "llama = AutoModelForCausalLM.from_pretrained(  # type: ignore\n",
    "                f\"EleutherAI/pythia-{model_size}\",\n",
    "                device_map={\"\": \"cuda:0\"},\n",
    "                revision=\"main\",\n",
    "                torch_dtype=\"auto\",\n",
    "            )\n",
    "\n",
    "    \n",
    "latent=\"en\"\n",
    "if type == 'logit':\n",
    "    llama_lens = LogitLens.from_model(llama).to(\"cuda\")\n",
    "    out_dir = f'./visuals/logit'\n",
    "if type == 'tuned':\n",
    "    llama_lens = TunedLens.from_model_and_pretrained(llama,f\"EleutherAI/{model_size}\").to(\"cuda\")\n",
    "    out_dir = f'./visuals/tuned'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"EleutherAI/{model_size}\",local_files_only=\"true\",cache_dir=\"/mnt/ssd-1/hf_cache/\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45e052",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unemb = nn.Sequential(llama.model.norm, llama.lm_head)\n",
    "print(unemb)\n",
    "# prepare for energy plots\n",
    "U = list(unemb[1].parameters())[0].detach().cpu().float()\n",
    "weights = list(unemb[0].parameters())[0].detach().cpu().float()\n",
    "print(f'U {U.shape} weights {weights.unsqueeze(0).shape}')\n",
    "U_weighted = U.clone() \n",
    "#U_weighted = U_weighted / ((U_weighted**2).mean(dim=1, keepdim=True))**0.5\n",
    "U_weighted *= weights.unsqueeze(0)\n",
    "U_normalized = U_weighted / ((U_weighted**2).sum(dim=1, keepdim=True))**0.5\n",
    "v = U.shape[0]\n",
    "TT = U_normalized.T @ U_normalized\n",
    "avgUU = (((U_normalized.T @ U_normalized)**2).sum() / v**2)**0.5\n",
    "print(avgUU.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686a8d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for idx, word in enumerate(df_en_target['word_translation']):\n",
    "    if word in tokenizer.get_vocab() or '▁'+word in tokenizer.get_vocab():\n",
    "        count += 1\n",
    "\n",
    "print(f'for {target_lang} {count} of {len(df_en_target)} are single tokens')\n",
    "\n",
    "if input_lang == target_lang:\n",
    "    df_en_target_input = df_en_target.copy()\n",
    "    if latent == 'zh':\n",
    "      df_en_target_input = df_en_target.copy()\n",
    "      chinese = chinese.loc[chinese[\"word_original\"].isin(df_en_input[\"word_original\"])]\n",
    "      df_en_target_input= df_en_target_input.loc[df_en_target_input[\"word_original\"].isin(chinese[\"word_original\"])]\n",
    "      chinese.sort_values(by=\"word_original\", inplace=True)\n",
    "      df_en_target_input.sort_values(by=\"word_original\", inplace=True)\n",
    "      chinese.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input[\"word_original\"] = chinese[\"word_translation\"]\n",
    "    df_en_target_input.rename(columns={'word_original': latent, \n",
    "                                f'word_translation': target_lang if target_lang != latent else f'{latent}_tgt'}, \n",
    "                                inplace=True)\n",
    "    \n",
    "else:\n",
    "    df_en_target_input = df_en_target.merge(df_en_input, on=['word_original'], suffixes=(f'_{target_lang}', f'_{input_lang}'))\n",
    "    if latent == 'zh':\n",
    "      chinese = chinese.loc[chinese[\"word_original\"].isin(df_en_target_input[\"word_original\"])]\n",
    "      df_en_target_input= df_en_target_input.loc[df_en_target_input[\"word_original\"].isin(chinese[\"word_original\"])]\n",
    "      chinese.sort_values(by=\"word_original\", inplace=True)\n",
    "      df_en_target_input.sort_values(by=\"word_original\", inplace=True)\n",
    "      chinese.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input[\"word_original\"] = chinese[\"word_translation\"]\n",
    "\n",
    "    df_en_target_input.rename(columns={'word_original': latent, \n",
    "                                f'word_translation_{target_lang}': target_lang if target_lang != latent else f'{latent}_tgt', \n",
    "                                f'word_translation_{input_lang}': input_lang if input_lang != latent else f'{latent}_in'}, \n",
    "                                inplace=True)\n",
    "# delete all rows where en is contained in de or fr\n",
    "if target_lang != latent:\n",
    "    for i, row in df_en_target_input.iterrows():\n",
    "        if row[latent].lower() in row[target_lang].lower():\n",
    "            df_en_target_input.drop(i, inplace=True)\n",
    "\n",
    "print(f'final length of df_{latent}_{target_lang}_{input_lang}: {len(df_en_target_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a52d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_prefixes(token_str: str):\n",
    "    n = len(token_str)\n",
    "    tokens = [token_str[:i] for i in range(1, n+1)]\n",
    "    return tokens \n",
    "\n",
    "def add_spaces(tokens):\n",
    "    return ['▁' + t for t in tokens] + tokens\n",
    "\n",
    "def capitalizations(tokens):\n",
    "    return list(set(tokens))\n",
    "\n",
    "def unicode_prefix_tokid(zh_char = \"云\", tokenizer=tokenizer):\n",
    "    start = zh_char.encode().__str__()[2:-1].split('\\\\x')[1]\n",
    "    unicode_format = '<0x%s>'\n",
    "    start_key = unicode_format%start.upper()\n",
    "    if start_key in tokenizer.get_vocab():\n",
    "        return tokenizer.get_vocab()[start_key]\n",
    "    return None\n",
    "\n",
    "def process_tokens(token_str: str, tokenizer, lang):\n",
    "    with_prefixes = token_prefixes(token_str)\n",
    "    with_spaces = add_spaces(with_prefixes)\n",
    "    with_capitalizations = capitalizations(with_spaces)\n",
    "    final_tokens = []\n",
    "    for tok in with_capitalizations:\n",
    "        if tok in tokenizer.get_vocab():\n",
    "            final_tokens.append(tokenizer.get_vocab()[tok])\n",
    "    if lang in ['zh', 'ru']:\n",
    "        tokid = unicode_prefix_tokid(token_str, tokenizer)\n",
    "        if tokid is not None:\n",
    "            final_tokens.append(tokid)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7bb78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "id2voc = {id:voc for voc, id in tokenizer.get_vocab().items()}\n",
    "def get_tokens(token_ids, id2voc=id2voc):\n",
    "    return [id2voc[tokid] for tokid in token_ids]\n",
    "\n",
    "def compute_entropy(probas):\n",
    "    return (-probas*torch.log2(probas)).sum(dim=-1)\n",
    "\n",
    "lang2name = {'fr': 'Français', 'de': 'Deutsch', 'ru': 'Русский', 'en': 'English', 'zh': '中文'}\n",
    "def sample(df, ind, k=5, tokenizer=tokenizer, lang1='fr', lang2='de', lang_latent=latent):\n",
    "    df = df.reset_index(drop=True)\n",
    "    temp = df[df.index!=ind]\n",
    "    sample = pd.concat([temp.sample(k-1), df[df.index==ind]], axis=0)\n",
    "    prompt = \"\"\n",
    "    for idx, (df_idx, row) in enumerate(sample.iterrows()):\n",
    "        if idx < k-1:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"{row[lang2]}\"\\n'\n",
    "        else:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"'\n",
    "            in_token_str = row[lang1]\n",
    "            out_token_str = row[lang2]\n",
    "            out_token_id = process_tokens(out_token_str, tokenizer, lang2)\n",
    "            latent_token_str = row[lang_latent]\n",
    "            latent_token_id = process_tokens(latent_token_str, tokenizer, latent)\n",
    "            intersection = set(out_token_id).intersection(set(latent_token_id))\n",
    "            if len(out_token_id) == 0 or len(latent_token_id) == 0:\n",
    "                yield None\n",
    "            if lang2 != latent and len(intersection) > 0:\n",
    "                yield None\n",
    "            yield {'prompt': prompt, \n",
    "                'out_token_id': out_token_id, \n",
    "                'out_token_str': out_token_str,\n",
    "                'latent_token_id': latent_token_id, \n",
    "                'latent_token_str': latent_token_str, \n",
    "                'in_token_str': in_token_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500641f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for ind in tqdm(range(len(df_en_target_input))):\n",
    "    d = next(sample(df_en_target_input, ind, lang1=input_lang, lang2=target_lang))\n",
    "    if d is None:\n",
    "        continue\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786ab1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "df.to_csv(f'{out_dir}/translation/{model_size}_{input_lang}_{target_lang}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e12355",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa55b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logits(model, prompt,lens):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    output = llama(inputs.input_ids, output_hidden_states=True)\n",
    "    hidden_states = output.hidden_states[:-1]\n",
    "    final_lps = output.logits.log_softmax(dim=-1)\n",
    "    tensors=[]\n",
    "    hd= []\n",
    "    for i in range(len(model.model.layers)):\n",
    "        h = hidden_states[i].squeeze(0)\n",
    "        tensors+=[lens(h, idx=i).detach().cpu()]\n",
    "        hd+=[lens.transform_hidden(h, idx=i).detach().cpu()]\n",
    "    tensors= torch.stack(tensors)  \n",
    "    hidden_states = torch.stack(hd)\n",
    "    return tensors,hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25241c49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_token_probs = []\n",
    "latent_token_probs = []\n",
    "out_token_probs = []\n",
    "entropy = []\n",
    "energy = []\n",
    "latents_all = []\n",
    "\n",
    "for idx, d in tqdm(enumerate(dataset)):\n",
    "    logits,latents = get_logits(llama, d['prompt'],llama_lens)\n",
    "    last = logits[:, -1, :].float().softmax(dim=-1).detach().cpu()\n",
    "    latent_token_probs += [last[:, torch.tensor(d['latent_token_id'])].sum(dim=-1)]\n",
    "    out_token_probs += [last[:, torch.tensor(d['out_token_id'])].sum(dim=-1)]\n",
    "    entropy += [compute_entropy(last)]\n",
    "    latents_all += [latents[:, -1, :].float().detach().cpu().clone()]\n",
    "    latents_normalized = latents[:, -1, :].float()\n",
    "    latents_normalized = latents_normalized / (((latents_normalized**2).mean(dim=-1, keepdim=True))**0.5)\n",
    "    latents_normalized /= (latents_normalized.norm(dim=-1, keepdim=True))\n",
    "    norm = ((U_normalized @ latents_normalized.T)**2).mean(dim=0)**0.5\n",
    "    energy += [norm/avgUU]\n",
    "\n",
    "latent_token_probs = torch.stack(latent_token_probs)\n",
    "out_token_probs = torch.stack(out_token_probs)\n",
    "entropy = torch.stack(entropy)\n",
    "energy = torch.stack(energy)\n",
    "latents = torch.stack(latents_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ef0b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10}\n",
    "fig, ax, ax2 = plot_ci_plus_heatmap(latent_token_probs, entropy, latent, color='tab:orange', tik_step=size2tik[model_size], do_colorbar=True, #, do_colorbar=(model_size=='70b'),\n",
    "nums=[.99, 0.18, 0.025, 0.6])\n",
    "plot_ci(ax2, out_token_probs, target_lang, color='tab:blue', do_lines=False)\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('probability')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "ax2.set_ylim(0, 1)\n",
    "# make xticks start from 1\n",
    "# put legend on the top left\n",
    "ax2.legend(loc='upper left')\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "\n",
    "plt.savefig(f'{out_dir}/translation/pythia_{model_size}_{input_lang}_{target_lang}_probas_ent.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c50e4a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10}\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(5,3))\n",
    "plot_ci(ax2, energy, 'energy', color='tab:green', do_lines=True, tik_step=size2tik[model_size])\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('energy')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "plt.savefig(f'{out_dir}/translation/pythia_{model_size}_{input_lang}_{target_lang}_energy.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f66f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.153175,
   "end_time": "2024-03-02T14:23:25.267552",
   "environment_variables": {},
   "exception": true,
   "input_path": "Translation_pythia.ipynb",
   "output_path": "visuals/executed_notebooks/pythia_Translation_tuned_6.9b_fr_ru.ipynb",
   "parameters": {
    "input_lang": "fr",
    "model_size": "6.9b",
    "target_lang": "ru",
    "type": "tuned"
   },
   "start_time": "2024-03-02T14:23:16.114377",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}