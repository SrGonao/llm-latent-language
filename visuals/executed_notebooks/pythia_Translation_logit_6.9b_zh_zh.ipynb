{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38b8182",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e266eca-b017-461f-9be4-bec02cae9b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:12.450105Z",
     "iopub.status.busy": "2024-03-02T14:56:12.449919Z",
     "iopub.status.idle": "2024-03-02T14:56:12.464155Z",
     "shell.execute_reply": "2024-03-02T14:56:12.463785Z"
    },
    "papermill": {
     "duration": 0.024323,
     "end_time": "2024-03-02T14:56:12.465217",
     "exception": false,
     "start_time": "2024-03-02T14:56:12.440894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae8a7a9-6902-424e-8e66-b107fccb361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:12.472109Z",
     "iopub.status.busy": "2024-03-02T14:56:12.471810Z",
     "iopub.status.idle": "2024-03-02T14:56:15.617883Z",
     "shell.execute_reply": "2024-03-02T14:56:15.616827Z"
    },
    "papermill": {
     "duration": 3.150679,
     "end_time": "2024-03-02T14:56:15.619195",
     "exception": false,
     "start_time": "2024-03-02T14:56:12.468516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from llamawrapper import load_unemb_only, LlamaHelper\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from utils import plot_ci, plot_ci_plus_heatmap\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "sys.path.append('../tuned-lens')\n",
    "from tuned_lens.nn.lenses import TunedLens,LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb93987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:15.630228Z",
     "iopub.status.busy": "2024-03-02T14:56:15.629736Z",
     "iopub.status.idle": "2024-03-02T14:56:15.666132Z",
     "shell.execute_reply": "2024-03-02T14:56:15.665413Z"
    },
    "papermill": {
     "duration": 0.041378,
     "end_time": "2024-03-02T14:56:15.667315",
     "exception": false,
     "start_time": "2024-03-02T14:56:15.625937",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "input_lang = 'zh'\n",
    "target_lang = 'fr'\n",
    "model_size = '6.9b'\n",
    "prefix = \"./data/langs/\"\n",
    "type = 'tuned-chinese-en'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daffdfaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:15.675172Z",
     "iopub.status.busy": "2024-03-02T14:56:15.674990Z",
     "iopub.status.idle": "2024-03-02T14:56:15.708463Z",
     "shell.execute_reply": "2024-03-02T14:56:15.707779Z"
    },
    "papermill": {
     "duration": 0.038347,
     "end_time": "2024-03-02T14:56:15.709741",
     "exception": false,
     "start_time": "2024-03-02T14:56:15.671394",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_size = \"6.9b\"\n",
    "target_lang = \"zh\"\n",
    "input_lang = \"zh\"\n",
    "type = \"logit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb0899b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:15.717135Z",
     "iopub.status.busy": "2024-03-02T14:56:15.716473Z",
     "iopub.status.idle": "2024-03-02T14:56:15.747591Z",
     "shell.execute_reply": "2024-03-02T14:56:15.746821Z"
    },
    "papermill": {
     "duration": 0.035798,
     "end_time": "2024-03-02T14:56:15.748745",
     "exception": false,
     "start_time": "2024-03-02T14:56:15.712947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_en_input = pd.read_csv(f'{prefix}{input_lang}/clean.csv').reindex()\n",
    "df_en_target = pd.read_csv(f'{prefix}{target_lang}/clean.csv').reindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33747213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:15.756075Z",
     "iopub.status.busy": "2024-03-02T14:56:15.755525Z",
     "iopub.status.idle": "2024-03-02T14:56:18.833291Z",
     "shell.execute_reply": "2024-03-02T14:56:18.832251Z"
    },
    "papermill": {
     "duration": 3.082909,
     "end_time": "2024-03-02T14:56:18.834915",
     "exception": false,
     "start_time": "2024-03-02T14:56:15.752006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                    | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 1/2 [00:01<00:01,  1.77s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.13s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.22s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "llama = AutoModelForCausalLM.from_pretrained(  # type: ignore\n",
    "                f\"EleutherAI/pythia-{model_size}\",\n",
    "                device_map={\"\": \"cuda:0\"},\n",
    "                revision=\"main\",\n",
    "                torch_dtype=\"auto\",\n",
    "            )\n",
    "\n",
    "    \n",
    "latent=\"en\"\n",
    "if type == 'logit':\n",
    "    llama_lens = LogitLens.from_model(llama).to(\"cuda\")\n",
    "    out_dir = f'./visuals/logit'\n",
    "if type == 'tuned':\n",
    "    llama_lens = TunedLens.from_model_and_pretrained(llama,f\"EleutherAI/{model_size}-deduped\").to(\"cuda\")\n",
    "    out_dir = f'./visuals/tuned'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"EleutherAI/pythia-{model_size}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c85805",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc45e052",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T14:56:18.842980Z",
     "iopub.status.busy": "2024-03-02T14:56:18.842789Z",
     "iopub.status.idle": "2024-03-02T14:56:19.071618Z",
     "shell.execute_reply": "2024-03-02T14:56:19.070533Z"
    },
    "papermill": {
     "duration": 0.233725,
     "end_time": "2024-03-02T14:56:19.072525",
     "exception": true,
     "start_time": "2024-03-02T14:56:18.838800",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPTNeoXForCausalLM' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unemb \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[43mllama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mnorm, llama\u001b[38;5;241m.\u001b[39mlm_head)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(unemb)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# prepare for energy plots\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPTNeoXForCausalLM' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "unemb = nn.Sequential(llama.model.norm, llama.lm_head)\n",
    "print(unemb)\n",
    "# prepare for energy plots\n",
    "U = list(unemb[1].parameters())[0].detach().cpu().float()\n",
    "weights = list(unemb[0].parameters())[0].detach().cpu().float()\n",
    "print(f'U {U.shape} weights {weights.unsqueeze(0).shape}')\n",
    "U_weighted = U.clone() \n",
    "#U_weighted = U_weighted / ((U_weighted**2).mean(dim=1, keepdim=True))**0.5\n",
    "U_weighted *= weights.unsqueeze(0)\n",
    "U_normalized = U_weighted / ((U_weighted**2).sum(dim=1, keepdim=True))**0.5\n",
    "v = U.shape[0]\n",
    "TT = U_normalized.T @ U_normalized\n",
    "avgUU = (((U_normalized.T @ U_normalized)**2).sum() / v**2)**0.5\n",
    "print(avgUU.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686a8d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for idx, word in enumerate(df_en_target['word_translation']):\n",
    "    if word in tokenizer.get_vocab() or '▁'+word in tokenizer.get_vocab():\n",
    "        count += 1\n",
    "\n",
    "print(f'for {target_lang} {count} of {len(df_en_target)} are single tokens')\n",
    "\n",
    "if input_lang == target_lang:\n",
    "    df_en_target_input = df_en_target.copy()\n",
    "    if latent == 'zh':\n",
    "      df_en_target_input = df_en_target.copy()\n",
    "      chinese = chinese.loc[chinese[\"word_original\"].isin(df_en_input[\"word_original\"])]\n",
    "      df_en_target_input= df_en_target_input.loc[df_en_target_input[\"word_original\"].isin(chinese[\"word_original\"])]\n",
    "      chinese.sort_values(by=\"word_original\", inplace=True)\n",
    "      df_en_target_input.sort_values(by=\"word_original\", inplace=True)\n",
    "      chinese.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input[\"word_original\"] = chinese[\"word_translation\"]\n",
    "    df_en_target_input.rename(columns={'word_original': latent, \n",
    "                                f'word_translation': target_lang if target_lang != latent else f'{latent}_tgt'}, \n",
    "                                inplace=True)\n",
    "    \n",
    "else:\n",
    "    df_en_target_input = df_en_target.merge(df_en_input, on=['word_original'], suffixes=(f'_{target_lang}', f'_{input_lang}'))\n",
    "    if latent == 'zh':\n",
    "      chinese = chinese.loc[chinese[\"word_original\"].isin(df_en_target_input[\"word_original\"])]\n",
    "      df_en_target_input= df_en_target_input.loc[df_en_target_input[\"word_original\"].isin(chinese[\"word_original\"])]\n",
    "      chinese.sort_values(by=\"word_original\", inplace=True)\n",
    "      df_en_target_input.sort_values(by=\"word_original\", inplace=True)\n",
    "      chinese.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input[\"word_original\"] = chinese[\"word_translation\"]\n",
    "\n",
    "    df_en_target_input.rename(columns={'word_original': latent, \n",
    "                                f'word_translation_{target_lang}': target_lang if target_lang != latent else f'{latent}_tgt', \n",
    "                                f'word_translation_{input_lang}': input_lang if input_lang != latent else f'{latent}_in'}, \n",
    "                                inplace=True)\n",
    "# delete all rows where en is contained in de or fr\n",
    "if target_lang != latent:\n",
    "    for i, row in df_en_target_input.iterrows():\n",
    "        if row[latent].lower() in row[target_lang].lower():\n",
    "            df_en_target_input.drop(i, inplace=True)\n",
    "\n",
    "print(f'final length of df_{latent}_{target_lang}_{input_lang}: {len(df_en_target_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a52d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_prefixes(token_str: str):\n",
    "    n = len(token_str)\n",
    "    tokens = [token_str[:i] for i in range(1, n+1)]\n",
    "    return tokens \n",
    "\n",
    "def add_spaces(tokens):\n",
    "    return ['▁' + t for t in tokens] + tokens\n",
    "\n",
    "def capitalizations(tokens):\n",
    "    return list(set(tokens))\n",
    "\n",
    "def unicode_prefix_tokid(zh_char = \"云\", tokenizer=tokenizer):\n",
    "    start = zh_char.encode().__str__()[2:-1].split('\\\\x')[1]\n",
    "    unicode_format = '<0x%s>'\n",
    "    start_key = unicode_format%start.upper()\n",
    "    if start_key in tokenizer.get_vocab():\n",
    "        return tokenizer.get_vocab()[start_key]\n",
    "    return None\n",
    "\n",
    "def process_tokens(token_str: str, tokenizer, lang):\n",
    "    with_prefixes = token_prefixes(token_str)\n",
    "    with_spaces = add_spaces(with_prefixes)\n",
    "    with_capitalizations = capitalizations(with_spaces)\n",
    "    final_tokens = []\n",
    "    for tok in with_capitalizations:\n",
    "        if tok in tokenizer.get_vocab():\n",
    "            final_tokens.append(tokenizer.get_vocab()[tok])\n",
    "    if lang in ['zh', 'ru']:\n",
    "        tokid = unicode_prefix_tokid(token_str, tokenizer)\n",
    "        if tokid is not None:\n",
    "            final_tokens.append(tokid)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7bb78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "id2voc = {id:voc for voc, id in tokenizer.get_vocab().items()}\n",
    "def get_tokens(token_ids, id2voc=id2voc):\n",
    "    return [id2voc[tokid] for tokid in token_ids]\n",
    "\n",
    "def compute_entropy(probas):\n",
    "    return (-probas*torch.log2(probas)).sum(dim=-1)\n",
    "\n",
    "lang2name = {'fr': 'Français', 'de': 'Deutsch', 'ru': 'Русский', 'en': 'English', 'zh': '中文'}\n",
    "def sample(df, ind, k=5, tokenizer=tokenizer, lang1='fr', lang2='de', lang_latent=latent):\n",
    "    df = df.reset_index(drop=True)\n",
    "    temp = df[df.index!=ind]\n",
    "    sample = pd.concat([temp.sample(k-1), df[df.index==ind]], axis=0)\n",
    "    prompt = \"\"\n",
    "    for idx, (df_idx, row) in enumerate(sample.iterrows()):\n",
    "        if idx < k-1:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"{row[lang2]}\"\\n'\n",
    "        else:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"'\n",
    "            in_token_str = row[lang1]\n",
    "            out_token_str = row[lang2]\n",
    "            out_token_id = process_tokens(out_token_str, tokenizer, lang2)\n",
    "            latent_token_str = row[lang_latent]\n",
    "            latent_token_id = process_tokens(latent_token_str, tokenizer, latent)\n",
    "            intersection = set(out_token_id).intersection(set(latent_token_id))\n",
    "            if len(out_token_id) == 0 or len(latent_token_id) == 0:\n",
    "                yield None\n",
    "            if lang2 != latent and len(intersection) > 0:\n",
    "                yield None\n",
    "            yield {'prompt': prompt, \n",
    "                'out_token_id': out_token_id, \n",
    "                'out_token_str': out_token_str,\n",
    "                'latent_token_id': latent_token_id, \n",
    "                'latent_token_str': latent_token_str, \n",
    "                'in_token_str': in_token_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500641f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for ind in tqdm(range(len(df_en_target_input))):\n",
    "    d = next(sample(df_en_target_input, ind, lang1=input_lang, lang2=target_lang))\n",
    "    if d is None:\n",
    "        continue\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786ab1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "df.to_csv(f'{out_dir}/translation/{model_size}_{input_lang}_{target_lang}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e12355",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa55b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logits(model, prompt,lens):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    output = llama(inputs.input_ids, output_hidden_states=True)\n",
    "    hidden_states = output.hidden_states[:-1]\n",
    "    final_lps = output.logits.log_softmax(dim=-1)\n",
    "    tensors=[]\n",
    "    hd= []\n",
    "    for i in range(len(model.model.layers)):\n",
    "        h = hidden_states[i].squeeze(0)\n",
    "        tensors+=[lens(h, idx=i).detach().cpu()]\n",
    "        hd+=[lens.transform_hidden(h, idx=i).detach().cpu()]\n",
    "    tensors= torch.stack(tensors)  \n",
    "    hidden_states = torch.stack(hd)\n",
    "    return tensors,hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25241c49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_token_probs = []\n",
    "latent_token_probs = []\n",
    "out_token_probs = []\n",
    "entropy = []\n",
    "energy = []\n",
    "latents_all = []\n",
    "\n",
    "for idx, d in tqdm(enumerate(dataset)):\n",
    "    logits,latents = get_logits(llama, d['prompt'],llama_lens)\n",
    "    last = logits[:, -1, :].float().softmax(dim=-1).detach().cpu()\n",
    "    latent_token_probs += [last[:, torch.tensor(d['latent_token_id'])].sum(dim=-1)]\n",
    "    out_token_probs += [last[:, torch.tensor(d['out_token_id'])].sum(dim=-1)]\n",
    "    entropy += [compute_entropy(last)]\n",
    "    latents_all += [latents[:, -1, :].float().detach().cpu().clone()]\n",
    "    latents_normalized = latents[:, -1, :].float()\n",
    "    latents_normalized = latents_normalized / (((latents_normalized**2).mean(dim=-1, keepdim=True))**0.5)\n",
    "    latents_normalized /= (latents_normalized.norm(dim=-1, keepdim=True))\n",
    "    norm = ((U_normalized @ latents_normalized.T)**2).mean(dim=0)**0.5\n",
    "    energy += [norm/avgUU]\n",
    "\n",
    "latent_token_probs = torch.stack(latent_token_probs)\n",
    "out_token_probs = torch.stack(out_token_probs)\n",
    "entropy = torch.stack(entropy)\n",
    "energy = torch.stack(energy)\n",
    "latents = torch.stack(latents_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ef0b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10}\n",
    "fig, ax, ax2 = plot_ci_plus_heatmap(latent_token_probs, entropy, latent, color='tab:orange', tik_step=size2tik[model_size], do_colorbar=True, #, do_colorbar=(model_size=='70b'),\n",
    "nums=[.99, 0.18, 0.025, 0.6])\n",
    "plot_ci(ax2, out_token_probs, target_lang, color='tab:blue', do_lines=False)\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('probability')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "ax2.set_ylim(0, 1)\n",
    "# make xticks start from 1\n",
    "# put legend on the top left\n",
    "ax2.legend(loc='upper left')\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "\n",
    "plt.savefig(f'{out_dir}/translation/pythia_{model_size}_{input_lang}_{target_lang}_probas_ent.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c50e4a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10}\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(5,3))\n",
    "plot_ci(ax2, energy, 'energy', color='tab:green', do_lines=True, tik_step=size2tik[model_size])\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('energy')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "plt.savefig(f'{out_dir}/translation/pythia_{model_size}_{input_lang}_{target_lang}_energy.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f66f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.498328,
   "end_time": "2024-03-02T14:56:20.094388",
   "environment_variables": {},
   "exception": true,
   "input_path": "Translation_pythia.ipynb",
   "output_path": "visuals/executed_notebooks/pythia_Translation_logit_6.9b_zh_zh.ipynb",
   "parameters": {
    "input_lang": "zh",
    "model_size": "6.9b",
    "target_lang": "zh",
    "type": "logit"
   },
   "start_time": "2024-03-02T14:56:11.596060",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}