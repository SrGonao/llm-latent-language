{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57a6ca7",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [6]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e266eca-b017-461f-9be4-bec02cae9b28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:09:57.710432Z",
     "iopub.status.busy": "2024-03-02T15:09:57.710248Z",
     "iopub.status.idle": "2024-03-02T15:09:57.723892Z",
     "shell.execute_reply": "2024-03-02T15:09:57.723529Z"
    },
    "papermill": {
     "duration": 0.019057,
     "end_time": "2024-03-02T15:09:57.724933",
     "exception": false,
     "start_time": "2024-03-02T15:09:57.705876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae8a7a9-6902-424e-8e66-b107fccb361f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:09:57.734638Z",
     "iopub.status.busy": "2024-03-02T15:09:57.734469Z",
     "iopub.status.idle": "2024-03-02T15:10:00.748813Z",
     "shell.execute_reply": "2024-03-02T15:10:00.747683Z"
    },
    "papermill": {
     "duration": 3.019444,
     "end_time": "2024-03-02T15:10:00.750662",
     "exception": false,
     "start_time": "2024-03-02T15:09:57.731218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import seaborn as sns\n",
    "from scipy.stats import bootstrap\n",
    "from utils import plot_ci, plot_ci_plus_heatmap\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# fix random seed\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "sys.path.append('../tuned-lens')\n",
    "from tuned_lens.nn.lenses import TunedLens,LogitLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb93987",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:10:00.758771Z",
     "iopub.status.busy": "2024-03-02T15:10:00.758379Z",
     "iopub.status.idle": "2024-03-02T15:10:00.790518Z",
     "shell.execute_reply": "2024-03-02T15:10:00.789807Z"
    },
    "papermill": {
     "duration": 0.037256,
     "end_time": "2024-03-02T15:10:00.791717",
     "exception": false,
     "start_time": "2024-03-02T15:10:00.754461",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "input_lang = 'zh'\n",
    "target_lang = 'fr'\n",
    "model_size = '12b'\n",
    "prefix = \"./data/langs/\"\n",
    "type = 'tuned'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd5e87d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:10:00.799202Z",
     "iopub.status.busy": "2024-03-02T15:10:00.798663Z",
     "iopub.status.idle": "2024-03-02T15:10:00.825569Z",
     "shell.execute_reply": "2024-03-02T15:10:00.824874Z"
    },
    "papermill": {
     "duration": 0.03189,
     "end_time": "2024-03-02T15:10:00.826805",
     "exception": false,
     "start_time": "2024-03-02T15:10:00.794915",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "model_size = \"12b\"\n",
    "target_lang = \"ru\"\n",
    "input_lang = \"zh\"\n",
    "type = \"logit\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccb0899b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:10:00.833884Z",
     "iopub.status.busy": "2024-03-02T15:10:00.833318Z",
     "iopub.status.idle": "2024-03-02T15:10:00.863954Z",
     "shell.execute_reply": "2024-03-02T15:10:00.863346Z"
    },
    "papermill": {
     "duration": 0.035319,
     "end_time": "2024-03-02T15:10:00.865155",
     "exception": false,
     "start_time": "2024-03-02T15:10:00.829836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_en_input = pd.read_csv(f'{prefix}{input_lang}/clean.csv').reindex()\n",
    "df_en_target = pd.read_csv(f'{prefix}{target_lang}/clean.csv').reindex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b259506f",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33747213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T15:10:00.872151Z",
     "iopub.status.busy": "2024-03-02T15:10:00.871713Z",
     "iopub.status.idle": "2024-03-02T15:10:02.970166Z",
     "shell.execute_reply": "2024-03-02T15:10:02.968690Z"
    },
    "papermill": {
     "duration": 2.103162,
     "end_time": "2024-03-02T15:10:02.971531",
     "exception": true,
     "start_time": "2024-03-02T15:10:00.868369",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                    | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Loading checkpoint shards:   0%|                                                                                                                                                                                    | 0/3 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 66.31 MiB is free. Process 1008855 has 41.07 GiB memory in use. Process 1033220 has 6.40 GiB memory in use. Of the allocated memory 6.15 GiB is allocated by PyTorch, and 758.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pythia \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m      2\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEleutherAI/pythia-\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_size\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m latent\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    560\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m )\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/transformers/modeling_utils.py:3502\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3494\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3495\u001b[0m     (\n\u001b[1;32m   3496\u001b[0m         model,\n\u001b[1;32m   3497\u001b[0m         missing_keys,\n\u001b[1;32m   3498\u001b[0m         unexpected_keys,\n\u001b[1;32m   3499\u001b[0m         mismatched_keys,\n\u001b[1;32m   3500\u001b[0m         offload_index,\n\u001b[1;32m   3501\u001b[0m         error_msgs,\n\u001b[0;32m-> 3502\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloaded_state_dict_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3506\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3510\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_fast_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fast_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3518\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3520\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   3521\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/transformers/modeling_utils.py:3926\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3924\u001b[0m                     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, state_dict)\n\u001b[1;32m   3925\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3926\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3929\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloaded_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3930\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3931\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3932\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3933\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3934\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffload_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3935\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3936\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstate_dict_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstate_dict_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3937\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3938\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3939\u001b[0m \u001b[43m            \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3940\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3941\u001b[0m \u001b[43m            \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3942\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3943\u001b[0m         error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_error_msgs\n\u001b[1;32m   3944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/transformers/modeling_utils.py:805\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, hf_quantizer, is_safetensors, keep_in_fp32_modules, unexpected_keys)\u001b[0m\n\u001b[1;32m    798\u001b[0m     state_dict_index \u001b[38;5;241m=\u001b[39m offload_weight(param, param_name, model, state_dict_folder, state_dict_index)\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m    800\u001b[0m     hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mrequires_parameters_quantization)\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m hf_quantizer\u001b[38;5;241m.\u001b[39mcheck_quantized_param(model, param, param_name, state_dict))\n\u001b[1;32m    803\u001b[0m ):\n\u001b[1;32m    804\u001b[0m     \u001b[38;5;66;03m# For backward compatibility with older versions of `accelerate` and for non-quantized params\u001b[39;00m\n\u001b[0;32m--> 805\u001b[0m     \u001b[43mset_module_tensor_to_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mset_module_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, param_device, state_dict, unexpected_keys)\n",
      "File \u001b[0;32m/mnt/ssd-1/gpaulo/miniconda3/envs/default/lib/python3.10/site-packages/accelerate/utils/modeling.py:384\u001b[0m, in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    382\u001b[0m             module\u001b[38;5;241m.\u001b[39m_parameters[tensor_name] \u001b[38;5;241m=\u001b[39m param_cls(new_value, requires_grad\u001b[38;5;241m=\u001b[39mold_value\u001b[38;5;241m.\u001b[39mrequires_grad)\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 384\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     new_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 47.54 GiB of which 66.31 MiB is free. Process 1008855 has 41.07 GiB memory in use. Process 1033220 has 6.40 GiB memory in use. Of the allocated memory 6.15 GiB is allocated by PyTorch, and 758.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "pythia = AutoModelForCausalLM.from_pretrained(  # type: ignore\n",
    "                f\"EleutherAI/pythia-{model_size}\",\n",
    "                device_map={\"\": \"cuda:0\"},\n",
    "                revision=\"main\",\n",
    "                torch_dtype=\"auto\",\n",
    "            )\n",
    "\n",
    "    \n",
    "latent=\"en\"\n",
    "if type == 'logit':\n",
    "    pythia_lens = LogitLens.from_model(pythia).to(\"cuda\")\n",
    "    out_dir = f'./visuals/logit'\n",
    "if type == 'tuned':\n",
    "    pythia_lens = TunedLens.from_model_and_pretrained(pythia,f\"EleutherAI/pythia-{model_size}-deduped\").to(\"cuda\")\n",
    "    out_dir = f'./visuals/tuned'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                f\"EleutherAI/pythia-{model_size}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45e052",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "unemb = nn.Sequential(pythia.gpt_neox.final_layer_norm, pythia.embed_out)\n",
    "print(unemb)\n",
    "# prepare for energy plots\n",
    "U = list(unemb[1].parameters())[0].detach().cpu().float()\n",
    "weights = list(unemb[0].parameters())[0].detach().cpu().float()\n",
    "print(f'U {U.shape} weights {weights.unsqueeze(0).shape}')\n",
    "U_weighted = U.clone() \n",
    "#U_weighted = U_weighted / ((U_weighted**2).mean(dim=1, keepdim=True))**0.5\n",
    "U_weighted *= weights.unsqueeze(0)\n",
    "U_normalized = U_weighted / ((U_weighted**2).sum(dim=1, keepdim=True))**0.5\n",
    "v = U.shape[0]\n",
    "TT = U_normalized.T @ U_normalized\n",
    "avgUU = (((U_normalized.T @ U_normalized)**2).sum() / v**2)**0.5\n",
    "print(avgUU.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686a8d5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for idx, word in enumerate(df_en_target['word_translation']):\n",
    "    if word in tokenizer.get_vocab() or '▁'+word in tokenizer.get_vocab():\n",
    "        count += 1\n",
    "\n",
    "print(f'for {target_lang} {count} of {len(df_en_target)} are single tokens')\n",
    "\n",
    "if input_lang == target_lang:\n",
    "    df_en_target_input = df_en_target.copy()\n",
    "    if latent == 'zh':\n",
    "      df_en_target_input = df_en_target.copy()\n",
    "      chinese = chinese.loc[chinese[\"word_original\"].isin(df_en_input[\"word_original\"])]\n",
    "      df_en_target_input= df_en_target_input.loc[df_en_target_input[\"word_original\"].isin(chinese[\"word_original\"])]\n",
    "      chinese.sort_values(by=\"word_original\", inplace=True)\n",
    "      df_en_target_input.sort_values(by=\"word_original\", inplace=True)\n",
    "      chinese.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input[\"word_original\"] = chinese[\"word_translation\"]\n",
    "    df_en_target_input.rename(columns={'word_original': latent, \n",
    "                                f'word_translation': target_lang if target_lang != latent else f'{latent}_tgt'}, \n",
    "                                inplace=True)\n",
    "    \n",
    "else:\n",
    "    df_en_target_input = df_en_target.merge(df_en_input, on=['word_original'], suffixes=(f'_{target_lang}', f'_{input_lang}'))\n",
    "    if latent == 'zh':\n",
    "      chinese = chinese.loc[chinese[\"word_original\"].isin(df_en_target_input[\"word_original\"])]\n",
    "      df_en_target_input= df_en_target_input.loc[df_en_target_input[\"word_original\"].isin(chinese[\"word_original\"])]\n",
    "      chinese.sort_values(by=\"word_original\", inplace=True)\n",
    "      df_en_target_input.sort_values(by=\"word_original\", inplace=True)\n",
    "      chinese.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input.reset_index(drop=True, inplace=True)\n",
    "      df_en_target_input[\"word_original\"] = chinese[\"word_translation\"]\n",
    "\n",
    "    df_en_target_input.rename(columns={'word_original': latent, \n",
    "                                f'word_translation_{target_lang}': target_lang if target_lang != latent else f'{latent}_tgt', \n",
    "                                f'word_translation_{input_lang}': input_lang if input_lang != latent else f'{latent}_in'}, \n",
    "                                inplace=True)\n",
    "# delete all rows where en is contained in de or fr\n",
    "if target_lang != latent:\n",
    "    for i, row in df_en_target_input.iterrows():\n",
    "        if row[latent].lower() in row[target_lang].lower():\n",
    "            df_en_target_input.drop(i, inplace=True)\n",
    "\n",
    "print(f'final length of df_{latent}_{target_lang}_{input_lang}: {len(df_en_target_input)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c0a52d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def token_prefixes(token_str: str):\n",
    "    n = len(token_str)\n",
    "    tokens = [token_str[:i] for i in range(1, n+1)]\n",
    "    return tokens \n",
    "\n",
    "def add_spaces(tokens):\n",
    "    return ['▁' + t for t in tokens] + tokens\n",
    "\n",
    "def capitalizations(tokens):\n",
    "    return list(set(tokens))\n",
    "\n",
    "def unicode_prefix_tokid(zh_char = \"云\", tokenizer=tokenizer):\n",
    "    start = zh_char.encode().__str__()[2:-1].split('\\\\x')[1]\n",
    "    unicode_format = '<0x%s>'\n",
    "    start_key = unicode_format%start.upper()\n",
    "    if start_key in tokenizer.get_vocab():\n",
    "        return tokenizer.get_vocab()[start_key]\n",
    "    return None\n",
    "\n",
    "def process_tokens(token_str: str, tokenizer, lang):\n",
    "    with_prefixes = token_prefixes(token_str)\n",
    "    with_spaces = add_spaces(with_prefixes)\n",
    "    with_capitalizations = capitalizations(with_spaces)\n",
    "    final_tokens = []\n",
    "    for tok in with_capitalizations:\n",
    "        if tok in tokenizer.get_vocab():\n",
    "            final_tokens.append(tokenizer.get_vocab()[tok])\n",
    "    if lang in ['zh', 'ru']:\n",
    "        tokid = unicode_prefix_tokid(token_str, tokenizer)\n",
    "        if tokid is not None:\n",
    "            final_tokens.append(tokid)\n",
    "    return final_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa7bb78",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "id2voc = {id:voc for voc, id in tokenizer.get_vocab().items()}\n",
    "def get_tokens(token_ids, id2voc=id2voc):\n",
    "    return [id2voc[tokid] for tokid in token_ids]\n",
    "\n",
    "def compute_entropy(probas):\n",
    "    return (-probas*torch.log2(probas)).sum(dim=-1)\n",
    "\n",
    "lang2name = {'fr': 'Français', 'de': 'Deutsch', 'ru': 'Русский', 'en': 'English', 'zh': '中文'}\n",
    "def sample(df, ind, k=5, tokenizer=tokenizer, lang1='fr', lang2='de', lang_latent=latent):\n",
    "    df = df.reset_index(drop=True)\n",
    "    temp = df[df.index!=ind]\n",
    "    sample = pd.concat([temp.sample(k-1), df[df.index==ind]], axis=0)\n",
    "    prompt = \"\"\n",
    "    for idx, (df_idx, row) in enumerate(sample.iterrows()):\n",
    "        if idx < k-1:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"{row[lang2]}\"\\n'\n",
    "        else:\n",
    "            prompt += f'{lang2name[lang1]}: \"{row[lang1]}\" - {lang2name[lang2]}: \"'\n",
    "            in_token_str = row[lang1]\n",
    "            out_token_str = row[lang2]\n",
    "            out_token_id = process_tokens(out_token_str, tokenizer, lang2)\n",
    "            latent_token_str = row[lang_latent]\n",
    "            latent_token_id = process_tokens(latent_token_str, tokenizer, latent)\n",
    "            intersection = set(out_token_id).intersection(set(latent_token_id))\n",
    "            if len(out_token_id) == 0 or len(latent_token_id) == 0:\n",
    "                yield None\n",
    "            if lang2 != latent and len(intersection) > 0:\n",
    "                yield None\n",
    "            yield {'prompt': prompt, \n",
    "                'out_token_id': out_token_id, \n",
    "                'out_token_str': out_token_str,\n",
    "                'latent_token_id': latent_token_id, \n",
    "                'latent_token_str': latent_token_str, \n",
    "                'in_token_str': in_token_str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500641f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for ind in tqdm(range(len(df_en_target_input))):\n",
    "    d = next(sample(df_en_target_input, ind, lang1=input_lang, lang2=target_lang))\n",
    "    if d is None:\n",
    "        continue\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d786ab1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dataset)\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "df.to_csv(f'{out_dir}/translation/pythia-{model_size}_{input_lang}_{target_lang}_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e12355",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"prompt\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa55b7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logits(model, prompt,lens):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
    "    output = pythia(inputs.input_ids, output_hidden_states=True)\n",
    "    hidden_states = output.hidden_states[:-1]\n",
    "    final_lps = output.logits.log_softmax(dim=-1)\n",
    "    tensors=[]\n",
    "    hd= []\n",
    "    for i in range(len(model.gpt_neox.layers)):\n",
    "        h = hidden_states[i].squeeze(0)\n",
    "        tensors+=[lens(h, idx=i).detach().cpu()]\n",
    "        hd+=[lens.transform_hidden(h, idx=i).detach().cpu()]\n",
    "    tensors= torch.stack(tensors)  \n",
    "    hidden_states = torch.stack(hd)\n",
    "    return tensors,hidden_states\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25241c49",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "in_token_probs = []\n",
    "latent_token_probs = []\n",
    "out_token_probs = []\n",
    "entropy = []\n",
    "energy = []\n",
    "latents_all = []\n",
    "\n",
    "for idx, d in tqdm(enumerate(dataset)):\n",
    "    logits,latents = get_logits(pythia, d['prompt'],pythia_lens)\n",
    "    last = logits[:, -1, :].float().softmax(dim=-1).detach().cpu()\n",
    "    latent_token_probs += [last[:, torch.tensor(d['latent_token_id'])].sum(dim=-1)]\n",
    "    out_token_probs += [last[:, torch.tensor(d['out_token_id'])].sum(dim=-1)]\n",
    "    entropy += [compute_entropy(last)]\n",
    "    latents_all += [latents[:, -1, :].float().detach().cpu().clone()]\n",
    "    latents_normalized = latents[:, -1, :].float()\n",
    "    latents_normalized = latents_normalized / (((latents_normalized**2).mean(dim=-1, keepdim=True))**0.5)\n",
    "    latents_normalized /= (latents_normalized.norm(dim=-1, keepdim=True))\n",
    "    norm = ((U_normalized @ latents_normalized.T)**2).mean(dim=0)**0.5\n",
    "    energy += [norm/avgUU]\n",
    "\n",
    "latent_token_probs = torch.stack(latent_token_probs)\n",
    "out_token_probs = torch.stack(out_token_probs)\n",
    "entropy = torch.stack(entropy)\n",
    "energy = torch.stack(energy)\n",
    "latents = torch.stack(latents_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ef0b2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10, '6.9b': 10,'12b': 10}\n",
    "fig, ax, ax2 = plot_ci_plus_heatmap(latent_token_probs, entropy, latent, color='tab:orange', tik_step=size2tik[model_size], do_colorbar=True, #, do_colorbar=(model_size=='70b'),\n",
    "nums=[.99, 0.18, 0.025, 0.6])\n",
    "plot_ci(ax2, out_token_probs, target_lang, color='tab:blue', do_lines=False)\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('probability')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "ax2.set_ylim(0, 1)\n",
    "# make xticks start from 1\n",
    "# put legend on the top left\n",
    "ax2.legend(loc='upper left')\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "\n",
    "plt.savefig(f'{out_dir}/translation/pythia_{model_size}_{input_lang}_{target_lang}_probas_ent.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c50e4a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size2tik = {'7b': 5, '13b': 5, '70b': 10,'12b': 10}\n",
    "\n",
    "fig, ax2 = plt.subplots(figsize=(5,3))\n",
    "plot_ci(ax2, energy, 'energy', color='tab:green', do_lines=True, tik_step=size2tik[model_size])\n",
    "ax2.set_xlabel('layer')\n",
    "ax2.set_ylabel('energy')\n",
    "if model_size == '7b':\n",
    "    ax2.set_xlim(0, out_token_probs.shape[1]+1)\n",
    "else:\n",
    "    ax2.set_xlim(0, round(out_token_probs.shape[1]/10)*10+1)\n",
    "os.makedirs(f'{out_dir}/translation', exist_ok=True)\n",
    "plt.savefig(f'{out_dir}/translation/pythia_{model_size}_{input_lang}_{target_lang}_energy.jpg', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0f66f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.688637,
   "end_time": "2024-03-02T15:10:04.594272",
   "environment_variables": {},
   "exception": true,
   "input_path": "Translation_pythia.ipynb",
   "output_path": "visuals/executed_notebooks/pythia_Translation_logit_12b_zh_ru.ipynb",
   "parameters": {
    "input_lang": "zh",
    "model_size": "12b",
    "target_lang": "ru",
    "type": "logit"
   },
   "start_time": "2024-03-02T15:09:56.905635",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}